{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d2f72d-4a26-4a4d-b944-c37d78c6499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import yfinance as yf\n",
    "#import pandas as pd\n",
    "#from newsapi import NewsApiClient\n",
    "#initializing the news API client\n",
    "#newsapi = NewsApiClient(api_key='')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09bf5858-11a7-44ae-a984-26a4540965ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3034c947-f579-4360-ba51-62de3ec279a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb8e226-e388-49a5-a296-a0370948e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc796c8-8ac9-4c89-8f99-91a8695b0632",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:\\\\ML PROJECTS\\\\Market_maestro\\\\datasets\\\\Sentiment_analysis_data\\\\data.csv'  # Update with the actual file path\n",
    "data = pd.read_csv(file_path, header=None, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8a9dff9-ea8d-4c4f-9a0d-1e839db30d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign column names\n",
    "data.columns = ['Sentiment', 'News_Headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab0766f1-4c82-4329-8c16-34e0e694560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # remove hashtags\n",
    "    text = text.lower()  # lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove non-alphabetical characters\n",
    "    \n",
    "    # Split text into tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back to string\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "410de42f-1645-4cd2-b670-4ffe4fe88fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816cf911-5219-4732-92c5-aad491138845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def cleanText(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # remove hashtags\n",
    "    text = text.lower()  # lowercase\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # remove non-alphabetical characters\n",
    "    \n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize each word\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845cca0f-95b7-4415-a141-546aa96dd7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA VISUALIZATION AND EXPLORATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce4a2b-789b-47d7-8d06-1197a61eda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1b21f-1381-4204-937f-e589de690331",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb69c13-665b-4418-af51-85c28b6d235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#having a few duplicates can provide more examples for the model,\n",
    "#which can help it learn better,\n",
    "#especially if the sentiment is balanced.\n",
    "#syntax to remove the duplicates\n",
    "#data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e27843-b771-4911-a437-cd2c5887746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403f716-d122-4904-bf9d-12fc88d1d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d614e2-b881-42bf-a4b6-3cb6d811aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign shape of the dataset\n",
    "data_shape = data.shape\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Display the shape and missing values\n",
    "data_shape, missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c128534c-ce1d-48fb-a235-1eec2246cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiments distribution\n",
    "sentiment_distribution = data['Sentiment'].value_counts()\n",
    "\n",
    "# Display the distribution\n",
    "sentiment_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2721a70f-89ab-4af9-8ed2-d31a68f0c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few news headlines\n",
    "headlines_sample = data['News_Headline'].sample(5, random_state=1)\n",
    "headlines_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f09f8-cf5f-4806-96ae-e89e1945437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=data, x='Sentiment', hue='Sentiment', palette='Set2', dodge=False, legend=False)  # Setting 'hue' and disabling the legend\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b4f91-f840-493c-950c-9759993f1ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How long the newshead line are by number of words\n",
    "# Plot the distribution of word counts without creating a new column\n",
    "plt.figure(figsize=(10, 6))\n",
    "word_counts = data['News_Headline'].apply(lambda x: len(str(x).split()))  # Calculate word count for each news headline\n",
    "plt.hist(word_counts, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Word Count in News Headlines')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac086f6f-4386-4515-ad7f-2fb132c3cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Cloud of Headlines\n",
    "#the higher the frequency the bigger the worg is presented in the wordcloud \n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine all headlines into a single string\n",
    "text = ' '.join(data['News_Headline'].astype(str))  # Convert to string and join\n",
    "\n",
    "# Create a word cloud of the headlines with a custom colormap\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis', max_words=200).generate(text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.title('Word Cloud of News Headlines', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506f921-ea6c-46e5-bc16-e8bb93d45e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same word cloud of the headlines with a circular shape\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='black', colormap='plasma', max_words=200, contour_color='white', contour_width=1).generate(text)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.title('Word Cloud of News Headlines', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c8b60c-9fb2-40f0-ab4d-6f26682dc34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of word count by sentiment with distinct colors\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a custom color palette\n",
    "custom_palette = {\n",
    "    'positive': 'lightgreen',\n",
    "    'neutral': 'lightblue',\n",
    "    'negative': 'lightcoral'\n",
    "}\n",
    "\n",
    "# Boxplot of word count by sentiment without creating a new column\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    x='Sentiment',\n",
    "    y=data['News_Headline'].apply(lambda x: len(str(x).split())),  # Calculate word count on the fly\n",
    "    hue='Sentiment',  # Assign the sentiment column to hue\n",
    "    data=data,\n",
    "    palette=custom_palette,\n",
    "    legend=False  # Disable the legend\n",
    ")\n",
    "plt.title('Word Count Distribution by Sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Word Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a00db5-4272-4261-9579-6cf4056d2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Visualization Report\n",
    "\n",
    "## Objective\n",
    "This visualization shows how the word count of news headlines varies by sentiment (positive, negative, neutral). The goal is to see if longer headlines are linked to certain sentiments.\n",
    "\n",
    "## Description of the Data\n",
    "The dataset contains news headlines and their sentiment labels. Each headline is marked as positive, negative, or neutral, indicating its tone.\n",
    "\n",
    "## Key Observations\n",
    "- Positive headlines generally have more words than negative and neutral ones.\n",
    "- There are some very long negative headlines that stand out as outliers.\n",
    "- Neutral headlines are usually shorter and more similar in length.\n",
    "\n",
    "## Insights Gained\n",
    "The results suggest that positive news often needs more words to explain the story, while neutral news tends to be shorter and to the point. The longer negative headlines may indicate more complex stories.\n",
    "\n",
    "## Next Steps\n",
    "Next, I plan to:\n",
    "- Create word clouds for each sentiment type to see common words.\n",
    "- Perform some statistical tests to compare word counts.\n",
    "- Look for connections between sentiment and other factors in the data.\n",
    "\n",
    "This will help me understand more about how news is reported and what it means for sentiment analysis.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Market Maestro",
   "language": "python",
   "name": "market_maestro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
